Sorting Algorithms Study Notes
1. Bubble Sort
Algorithm
Basic Idea: Repeatedly step through the list, compare adjacent elements and swap them if they're in the wrong order
Process: The largest element "bubbles" to the end with each pass
Implementation Approach
for i = 0 to n-1:
    for j = 0 to n-2-i:
        if arr[j] > arr[j+1]:
            swap(arr[j], arr[j+1])
Time Complexity
Best Case: O(n) - when array is already sorted (with optimization)
Average Case: O(n²)
Worst Case: O(n²) - when array is reverse sorted
Space Complexity
O(1) - in-place sorting algorithm
Characteristics
Stable: Yes (equal elements maintain relative order)
In-place: Yes
Adaptive: Yes (performs better on partially sorted arrays)
2. Insertion Sort
Algorithm
Basic Idea: Build the final sorted array one element at a time
Process: Take elements from unsorted portion and insert them at correct position in sorted portion
Implementation Approach
for i = 1 to n-1:
    key = arr[i]
    j = i-1
    while j >= 0 and arr[j] > key:
        arr[j+1] = arr[j]
        j = j-1
    arr[j+1] = key
Time Complexity
Best Case: O(n) - when array is already sorted
Average Case: O(n²)
Worst Case: O(n²) - when array is reverse sorted
Space Complexity
O(1) - in-place sorting algorithm
Characteristics
Stable: Yes
In-place: Yes
Adaptive: Yes
Good for: Small datasets and nearly sorted arrays
3. Selection Sort
Algorithm
Basic Idea: Repeatedly find the minimum element from unsorted portion and place it at the beginning
Process: Divide array into sorted and unsorted portions, progressively move minimum elements to sorted portion
Implementation Approach
for i = 0 to n-2:
    min_idx = i
    for j = i+1 to n-1:
        if arr[j] < arr[min_idx]:
            min_idx = j
    swap(arr[i], arr[min_idx])
Time Complexity
Best Case: O(n²)
Average Case: O(n²)
Worst Case: O(n²)
Always performs the same number of comparisons regardless of input
Space Complexity
O(1) - in-place sorting algorithm
Characteristics
Stable: No (can change relative order of equal elements)
In-place: Yes
Adaptive: No
Note: Makes minimum number of swaps (at most n-1)
4. Merge Sort
Algorithm
Basic Idea: Divide and conquer approach - divide array into halves, sort them, then merge
Process: Recursively divide until single elements, then merge back in sorted order
Implementation Approach
mergeSort(arr, left, right):
    if left < right:
        mid = (left + right) / 2
        mergeSort(arr, left, mid)
        mergeSort(arr, mid+1, right)
        merge(arr, left, mid, right)
Time Complexity
Best Case: O(n log n)
Average Case: O(n log n)
Worst Case: O(n log n)
Consistent performance across all cases
Space Complexity
O(n) - requires additional space for temporary arrays during merging
Characteristics
Stable: Yes
In-place: No (requires extra space)
Adaptive: No
Good for: Large datasets, linked lists, external sorting
5. Quick Sort
Algorithm
Basic Idea: Choose a pivot element, partition array around pivot, recursively sort subarrays
Process: Elements smaller than pivot go left, larger go right
Implementation Approach
quickSort(arr, low, high):
    if low < high:
        pivot_index = partition(arr, low, high)
        quickSort(arr, low, pivot_index-1)
        quickSort(arr, pivot_index+1, high)
Time Complexity
Best Case: O(n log n) - when pivot divides array into equal halves
Average Case: O(n log n)
Worst Case: O(n²) - when pivot is always smallest/largest element
Space Complexity
O(log n) - for recursion stack in average case
O(n) - in worst case
Characteristics
Stable: No (standard implementation)
In-place: Yes (ignoring recursion stack)
Adaptive: No
Good for: General purpose sorting, cache-friendly
Pivot Selection Strategies
First element, last element, middle element, random element, median-of-three
6. Counting Sort
Algorithm
Basic Idea: Count occurrences of each distinct element, then reconstruct sorted array
Process: Not comparison-based; uses element values as indices
Implementation Approach
1. Find the range of input (min and max values)
2. Create count array of size (max - min + 1)
3. Count occurrences of each element
4. Modify count array to store actual positions
5. Build output array using count array
Time Complexity
Best Case: O(n + k) where k is the range of input
Average Case: O(n + k)
Worst Case: O(n + k)
Space Complexity
O(k) - for count array, where k is the range of input values
Characteristics
Stable: Yes (when implemented properly)
In-place: No
Adaptive: No
Good for: Integer sorting with small range, as subroutine for radix sort
Limitations
Only works with integers or objects that can be mapped to integers
Inefficient when range k is much larger than n
Requires knowledge of input range
Comparison Summary
Algorithm	Time (Best)	Time (Avg)	Time (Worst)	Space	Stable	In-place
Bubble	O(n)	O(n²)	O(n²)	O(1)	Yes	Yes
Insertion	O(n)	O(n²)	O(n²)	O(1)	Yes	Yes
Selection	O(n²)	O(n²)	O(n²)	O(1)	No	Yes
Merge	O(n log n)	O(n log n)	O(n log n)	O(n)	Yes	No
Quick	O(n log n)	O(n log n)	O(n²)	O(log n)	No	Yes
Counting	O(n + k)	O(n + k)	O(n + k)	O(k)	Yes	No
When to Use Which Algorithm
Small datasets (n < 50): Insertion sort
Nearly sorted data: Insertion sort or bubble sort
Stability required: Merge sort or insertion sort
Memory constrained: Quick sort or heap sort
Worst-case performance critical: Merge sort
Integer data with small range: Counting sort
General purpose: Quick sort (most common choice)
Key Terms to Remember
Stable: Maintains relative order of equal elements
In-place: Uses O(1) extra space (excluding input)
Adaptive: Performs better on partially sorted input
Comparison-based: Uses comparisons between elements
Non-comparison: Uses other properties (like counting sort)
